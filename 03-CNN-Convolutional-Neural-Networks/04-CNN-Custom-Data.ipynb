{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d74f266-e609-4ca4-8db3-0482a24d1258",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c46b38-706b-4334-b575-f5edde946cfe",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "\n",
    "- Conv layer - 6 filters, 3x3 size, stride 1\n",
    "- Pooling layer - 2x2 size, stride 2\n",
    "- Conv layer - 16 filters, 3x3 size, stride 1\n",
    "- Pooling layer - 2x2 size, stride 2\n",
    "- Fully connected - 46,656* > 120 > 84 > 2\n",
    "*See calculation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cd2a5c8-b4fc-4e08-9003-48cc2cb06e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1: 222.0 > P1: 111.0 > C2: 109.0 > P2: 54.5 = 54 pixels per side\n",
      "Fully connected layers inputs = 46,656 > 54 pixels squared x number of channels\n"
     ]
    }
   ],
   "source": [
    "# Calculating fully connected layers input\n",
    "c1 = output_size(224,3,0,1)\n",
    "p1 = output_size(c1,2,0,2)\n",
    "c2 = output_size(p1,3,0,1)\n",
    "p2 = output_size(c2,2,0,2)\n",
    "\n",
    "print(f'C1: {c1} > P1: {p1} > C2: {c2} > P2: {p2} = {int(p2)} pixels per side')\n",
    "print(f'Fully connected layers inputs = {int(p2)**2*16:,} > {int(p2)} pixels squared x number of channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4463920-e767-4a76-ac95-3751f47c01af",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672fdd51-d0c1-4403-bd61-ecfee7d3c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models # add models to the list\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb509e38-5e36-4ae3-9fc7-c30dfd518065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_size(w,k,p,s):\n",
    "    '''\n",
    "    Calculates the output size of a convolution/pooling layer.\n",
    "    - w [int]: input volume\n",
    "    - k [int]: Kernel size\n",
    "    - p [int]: padding\n",
    "    - s [int]: stride\n",
    "    '''\n",
    "    return ((w-k+2*p)/s)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fcdd7-6517-43a6-9bb9-861ad9eaf51e",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc603908-f50d-49b7-aa40-966bbe83e10a",
   "metadata": {},
   "source": [
    "We will be creating two transform objects, one for train data, and one for test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61288355-8883-4fd4-8c29-743e51d33276",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b9482-ddb0-4451-873c-650461cd1868",
   "metadata": {},
   "source": [
    "Images do not need to be augmented in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dbcd185-9f8d-4efd-8623-dbaba509e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transfrom = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940d5a7-367c-4091-a131-e5ce2410dade",
   "metadata": {},
   "source": [
    "# Data sets and loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ff70d-7e45-40e1-a544-c826d7756caa",
   "metadata": {},
   "source": [
    "Loading the data can be done with the `datasets.ImageFolder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8680aa43-cbb1-418e-a67b-7500302002e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAT', 'DOG']\n",
      "Training images available: 18,743\n",
      "Testing images available:  6,251\n"
     ]
    }
   ],
   "source": [
    "root = '../Data/CATS_DOGS/'\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root,'train'),transform=train_transform)\n",
    "test_data = datasets.ImageFolder(os.path.join(root,'test'),transform=train_transform)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=10,shuffle=True,num_workers=3)\n",
    "test_loader = DataLoader(test_data,batch_size=10,shuffle=True)\n",
    "\n",
    "class_names = train_data.classes\n",
    "\n",
    "print(class_names)\n",
    "print(f'Training images available: {len(train_data):,}')\n",
    "print(f'Testing images available:  {len(test_data):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684f23e-e876-4d70-b45c-1d3ebf2b3022",
   "metadata": {},
   "source": [
    "# Reviewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7cbf017-7991-4049-a941-7db93d7b3d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Grab the first batch of images\n",
    "for images, labels in train_loader:\n",
    "    break\n",
    "\n",
    "# Print labels and classes\n",
    "print(f'labels')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
